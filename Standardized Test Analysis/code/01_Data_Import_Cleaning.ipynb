{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 1: Standardized Test Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Original Data\n",
    "\n",
    "The datasets included in the [`data/original`](./data/original) folder.\n",
    "\n",
    "| **Dataset** | **Description** |\n",
    "| --------------- | ----------------- |\n",
    "|[`act_2017.csv`](./data/original/act_2017.csv)| 2017 ACT Scores by State|\n",
    "|[`act_2018.csv`](./data/original/act_2018.csv)| 2018 ACT Scores by State|\n",
    "|[`act_2019.csv`](./data/original/act_2019.csv) | 2019 ACT Scores by State|\n",
    "|[`sat_2017.csv`](./data/original/sat_2017.csv)| 2017 SAT Scores by State|\n",
    "|[`sat_2018.csv`](./data/original/sat_2018.csv)| 2018 SAT Scores by State|\n",
    "|[`sat_2019.csv`](./data/original/sat_2019.csv) | 2019 SAT Scores by State|\n",
    "|[`sat_act_by_college.csv`](./data/original/sat_act_by_college.csv)| Ranges of Accepted ACT & SAT Student Scores by Colleges|\n",
    "\n",
    "### Cleaned Data\n",
    "\n",
    "In this notebook, ACT & SAT scores by state for the given years are concatenated to a single table per each test.\n",
    "\n",
    "- SAT and ACT for years 2017, 2018, 2019\n",
    "    - Sample output:\n",
    "    \n",
    "    \n",
    "| |year|state|sat_participation|sat_ebrw|sat_math|sat_total|act_participation|act_english|act_math|act_reading|act_science|act_composite|\n",
    "|--------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|\n",
    "|0|2017|Alabama|0.05|593|572|1165|1.00|18.9|18.4|19.7|19.4|19.2|\n",
    "|1|2017|Alaska|0.38|547|533|1080|0.65|18.7|19.8|20.4|19.9|19.8|\n",
    "|2|2017|Arizona|0.30|563|553|1116|0.62|18.6|19.8|20.1|19.8|19.7|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|Feature            |Type     | Dataset   | Description |\n",
    "|---                | ------  | --------  | ----------  |  \n",
    "|year               | object  | SAT & ACT | Year        |\n",
    "|state              | object  | SAT & ACT | The state where the test was taken. |\n",
    "|sat_participation  | float   | SAT       | Participation rate |\n",
    "|sat_ebrw           | int     | SAT       | Math and Evidence-Based Reading and Writing |\n",
    "|sat_math           | int     | SAT       | Math score |\n",
    "|sat_total          | int     | SAT       | Total score |\n",
    "|act_participation  | float   | ACT       | Participation rate |\n",
    "|act_english        | float   | ACT       | English score|\n",
    "|act_math           | float   | ACT       | Math score |\n",
    "|act_reading        | float   | ACT       | Reading score |\n",
    "|act_science        | float   | ACT       | Science score |\n",
    "|act_composite      | float   | ACT       | Composite score|\n",
    "|||*number of entries*: 153  ||\n",
    "|||*memory usage*: 14.5+ KB      ||\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| |Column|Dtype|Description|\n",
    "|---|------  |-----  |---------- |\n",
    "| 0   |state|object||\n",
    "| 1   |participation  |object||\n",
    "| 2   |ebrw           |int64 ||\n",
    "| 3   |math           |int64 ||\n",
    "| 4   |total          |int64 ||\n",
    "| 5   |year           |object||\n",
    "| 6   |test_type      |object||\n",
    "|number of entries| 155      |||\n",
    "|memory usage| 9.7+ KB      |||\n",
    "\n",
    "- ACT\n",
    "\n",
    "\n",
    "| |Column|Dtype|Description|\n",
    "|---|------  |-----  |---------- |\n",
    "| 0  | state          |object ||\n",
    "| 1  | participation  |float64||\n",
    "| 2  | english        |float64||\n",
    "| 3  | math           |float64||\n",
    "| 4  | reading        |float64||\n",
    "| 5  | science        |float64||\n",
    "| 6   |composite      |float64||\n",
    "| 7  | year           |object ||\n",
    "| 8   |test_type      |object ||\n",
    "|number of entries| 156 (includes National)|||\n",
    "|memory usage| 12.2+ KB      |||\n",
    "\n",
    "\n",
    "### Map Data\n",
    "\n",
    "- Geographic map data for states can be downloaded from this [source](https://www.arcgis.com/home/item.html?id=b07a9393ecbd430795a6f6218443dccc).\n",
    "\n",
    "## Sources\n",
    "\n",
    "- I consulted these two medium articles to figure out the setup for using geoPanda: [article 1](https://medium.com/@minaienick/why-you-should-be-using-geopandas-to-visualize-data-on-maps-aka-geo-visualization-fd1e3b6211b4), [article 2](https://medium.com/@erikgreenj/mapping-us-states-with-geopandas-made-simple-d7b6e66fa20d)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## required installations\n",
    "\n",
    "conda install -c anaconda beautifulsoup4\n",
    "conda install requests -y\n",
    "conda install -c conda-forge lxml -y\n",
    "conda install -c conda-forge geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements0.txt\n",
    "beautifulsoup4\n",
    "requests\n",
    "lxml\n",
    "geopy\n",
    "html5lib\n",
    "geopandas\n",
    "mapclassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# In notebook\n",
    "root_dir = Path().resolve().parents[1]  # Adjust if needed\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "from utils import *\n",
    "create_requirements_with_versions(quiet=True, upgrade=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce environment values\n",
    "\n",
    "original_data_path = '../data/original'\n",
    "cleaned_data_path = '../data/cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available dictionary for the functions to use\n",
    "\n",
    "us_state_abbrev = {'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ', 'Arkansas': 'AR', \n",
    "                   'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', \n",
    "                   'District of Columbia': 'DC', 'Florida': 'FL', 'Georgia': 'GA', 'Guam': 'GU', \n",
    "                   'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', \n",
    "                   'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': \n",
    "                   'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': \n",
    "                   'MS', 'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', \n",
    "                   'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', \n",
    "                   'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', \n",
    "                   'Northern Mariana Islands': 'MP', 'Ohio': 'OH', 'Oklahoma': 'OK', \n",
    "                   'Oregon': 'OR', 'Pennsylvania': 'PA', 'Puerto Rico': 'PR', 'Rhode Island': 'RI', \n",
    "                   'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', \n",
    "                   'Utah': 'UT', 'Vermont': 'VT', 'Virgin Islands': 'VI', 'Virginia': 'VA', \n",
    "                   'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY', 'Washington D.C.': 'DC'}\n",
    "\n",
    "\n",
    "colleges_states = {'Thomas Aquinas College': 'California',\n",
    " 'United States Merchant Maritime Academy': 'New York',\n",
    " 'United States Coast Guard Academy': 'Connecticut',\n",
    " 'Washington College': 'District of Columbia',\n",
    "'Southwestern University': 'Texas',\n",
    " 'Berry College': 'Georgia',\n",
    " 'Knox College': 'Illinois',\n",
    " 'Montclair State': 'New Jersey',\n",
    " 'East Carolina': 'North Carolina',\n",
    " 'Siena College': 'New York', \n",
    " 'Portland State': 'Oregon',\n",
    " 'Gallaudet University': 'District of Columbia'}\n",
    "\n",
    "mandatory_sat_states = ['Colorado', 'Connecticut', 'Delaware', 'Illinois', 'Michigan', 'New Hampshire', 'Rhode Island', 'West Virginia']\n",
    "\n",
    "mandatory_act_states = ['Hawaii', 'Kentucky', 'Louisiana', 'Nebraska ', 'North Carolina ', 'North Dakota ', 'Ohio', 'Oklahoma', 'Tennessee', 'Wyoming']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper lambda fuctions\n",
    "\n",
    "# checks to see if a given value is a string\n",
    "is_str = lambda input_value: type(input_value) == str \n",
    "\n",
    "# checks to see if a given string is a percentage\n",
    "is_percentage = lambda given_str: given_str.strip().endswith('%')\n",
    "\n",
    "# check to see if a given string includes integers or floats\n",
    "is_numerical = lambda given_str: given_str.replace('.', '').isdigit()\n",
    "\n",
    "# extract the year from a given string if it exists in a list\n",
    "get_year_from_str = lambda given_str: re.findall('(\\d{4})', given_str)\n",
    "\n",
    "get_list_of_csv_tables = lambda path: [f for f in os.listdir(path) if f.endswith('.csv') ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "\n",
    "def convert_percentage_float(pctg_str):\n",
    "    \"\"\"\n",
    "    Returns converted value of a percentage string to float.\n",
    "    \n",
    "    Parameters:\n",
    "        pctg_str (str): The string which to be converted.\n",
    "        \n",
    "    Returns:\n",
    "        pctg_float (float): The float value corresponding to the given input.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not is_str(pctg_str):\n",
    "        return pctg_str\n",
    "    \n",
    "    if not is_percentage(pctg_str):\n",
    "        return pctg_str\n",
    "    \n",
    "    pctg = pctg_str.strip().strip('%')\n",
    "    \n",
    "    if not is_numerical(pctg):\n",
    "        return pctg_str\n",
    "        \n",
    "    pctg_float = float(pctg)/100\n",
    "    return pctg_float\n",
    "\n",
    "\n",
    "\n",
    "def convert_camel_to_snake_case(text):\n",
    "    \"\"\"\n",
    "    The regular expression snippet is take from \n",
    "    https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case\n",
    "    with slight modification.\n",
    "    Parameters:\n",
    "        text (str): The string input in CamelCase.\n",
    "        \n",
    "    Returns:\n",
    "        out_text (str): The converted string in snake_case.\n",
    "    \"\"\"\n",
    "    pattern = re.compile('((?<=[a-z0-9])[A-Z]|(?!^)[A-Z](?=[a-z]))')\n",
    "    out_text = pattern.sub(r'_\\1', text.replace(' ', '_')).lower().replace('__', '_')\n",
    "    return out_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_100_university_ratings_in_states(path=cleaned_data_path):\n",
    "    \"\"\"\n",
    "    The code to use pd.read_html() is taken with modification from \n",
    "    # https://stackoverflow.com/questions/43590153/http-error-403-forbidden-when-reading-html\n",
    "    \"\"\"\n",
    "    url = \"https://www.topuniversities.com/where-to-study/north-america/united-states/ranked-top-100-us-universities\"\n",
    "\n",
    "    header = {\n",
    "      \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "      \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, headers=header)\n",
    "\n",
    "    try:\n",
    "        df1 = pd.read_html(r.text, skiprows=1,header=0)[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # Collect data\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        rows = soup.select(\".ranking-data-row\")\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            rank = row.select_one(\".rank\")\n",
    "            name = row.select_one(\".uni_name a\")\n",
    "            location = row.select_one(\".location\")\n",
    "        \n",
    "            if rank and name and location:\n",
    "                data.append({\n",
    "                    \"Rank\": rank.contents[0].strip().strip('='),\n",
    "                    \"University\": name.text.strip(),\n",
    "                    \"Location\": location.text.strip()\n",
    "                })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df1 = pd.DataFrame(data)\n",
    "    # df1['Rank'] = df1['Rank'].str.lstrip().str.strip('=').astype(int)\n",
    "    \n",
    "    df1['University'].replace({\n",
    "        'Stony Brook University': 'Stony Brook University SUNY',\n",
    "        'Harvard University': 'Harvard College',\n",
    "        'University of Wisconsin-Madison': 'University of Wisconsin, Madison',\n",
    "        'North Carolina State University': 'North Carolina State University, Raleigh',\n",
    "        'Indiana University Bloomington': 'Indiana University, Bloomington',\n",
    "        'University of Michigan': 'University of Michigan, Ann Arbor',\n",
    "        \"University of Hawai'i at Manoa\": 'University of Hawaii at Manoa',\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df1.to_csv(f'{path}/top_universities_rankings.csv', index = False)\n",
    "\n",
    "    url2 = \"https://www.topuniversities.com/university-rankings-articles/world-university-rankings/top-us-universities-state\"\n",
    "    \n",
    "    r2 = requests.get(url2, headers=header)\n",
    "    soup = BeautifulSoup(r2.text, 'html.parser')\n",
    "    #target = soup.find('h2', text='Top US universities by state')\n",
    "    target = soup.find(\"h2\", id=\"art_heading_10\")\n",
    "    texts = [sib.text.strip('\\xa0').strip() for sib in target.find_next_siblings()][:-4]\n",
    "    # states = [a.split(' – ')[0] for i, a in enumerate(texts) if i%2 == 0]\n",
    "    all_universities = [a for i, a in enumerate(texts) if i%2 == 1]\n",
    "    print(all_universities)\n",
    "    # out = [{'state': state, 'university': university.strip()} \n",
    "    #        for state, universities in zip(states, all_universities)\n",
    "    #        for university in universities.split(',')\n",
    "    #       ]\n",
    "    sus = [su.replace(' -', '-').replace('- ', '-').replace(':\\xa0', '-').split('-') for su in all_universities]\n",
    "    out = [{'state': su[0], 'university': su[1]} \n",
    "           for su in sus\n",
    "          ]\n",
    "    print(out)\n",
    "    df2 = pd.DataFrame(out)\n",
    "    \n",
    "    df2['university'].replace({\n",
    "        'Los Angeles (UCLA)': 'University of California, Los Angeles (UCLA)',\n",
    "        'Berkeley (UCB)': 'University of California Berkeley (UCB)',\n",
    "        'San Diego (UCSD)': 'University of California, San Diego (UCSD)',\n",
    "        'Davis (UCD)': 'University of California, Davis (UCD)',\n",
    "        'Santa Cruz (UCSC)': 'University of California, Santa Cruz (UCSC)',\n",
    "        'Riverside (UCR)': 'University of California, Riverside (UCR)',\n",
    "        'Irvine (UCI)': 'University of California, Irvine (UCI)',\n",
    "        'Santa Barbara (UCSB)': 'University of California, Santa Barbara (UCSB)',\n",
    "        'Harvard University': 'Harvard College',\n",
    "        \"University of Hawai'i at Manoa\": 'University of Hawaii at Manoa',\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df2.to_csv(f'{path}/states_universities.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_for_universities(list_of_university_names, path=cleaned_data_path):\n",
    "    \"\"\"\n",
    "    # https://towardsdatascience.com/geoparsing-with-python-c8f4c9f78940\n",
    "    Returns a dataframe with the list of universities and their coordinates\n",
    "    \"\"\"\n",
    "    \n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.exc import GeocoderTimedOut\n",
    "    geolocator = Nominatim(timeout=10, user_agent=\"http\")\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for school in list_of_university_names: \n",
    "        try:\n",
    "            location = geolocator.geocode(school)\n",
    "            if not location:\n",
    "                location = geolocator.geocode(school.split('(')[0])\n",
    "            if not location:\n",
    "                location = geolocator.geocode(school.rstrip(' SUNY'))\n",
    "            if not location:\n",
    "                location = geolocator.geocode(school.split(', ')[0])\n",
    "            if location:\n",
    "                out.update({school: location})\n",
    "                if location.raw['type'] not in ['university', 'college', 'school', 'museum']:\n",
    "                    print(school, location.raw['type'], sep=', ')\n",
    "        except GeocoderTimedOut as e:\n",
    "            print(\"Error: geocode failed on input %s with message %s\"%(school, e))\n",
    "\n",
    "    df = pd.DataFrame(out).T\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns = {'index': 'school',0: 'place', 1: 'coordinates'}, inplace=True)\n",
    "    df['state'] = df['place'].str.split(', ').str[-3]\n",
    "    df.loc[~df['state'].isin(us_state_abbrev.keys()), 'state'] = df['place'].str.split(', ').str[-2]\n",
    "    df = df[df['state'].isin(us_state_abbrev.keys())]\n",
    "    df.to_csv(f'{path}/university_coordinates_states.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_frame(df, pctg_columns = [], col_rename_dict = {}):\n",
    "    \"\"\"\n",
    "    Returns the modified version of the given dataframe after changing column names \n",
    "    and other required conversions.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input data frame\n",
    "        pctg_columns (list): A list of that include column names with percentage values.\n",
    "        col_rename_dict (dict): A dictionary including the original column names \n",
    "        and their correpsonding new names.\n",
    "        \n",
    "    Returns:\n",
    "        df (DataFrame): The modified/cleaned dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # drop columns with all nan entries\n",
    "    df.dropna(how='all', axis = 1, inplace = True)\n",
    "    \n",
    "    # drop rows with all nan entries\n",
    "    df.dropna(how='all', axis = 0, inplace = True)\n",
    "    \n",
    "    # convert column names to desired names \n",
    "    df.rename(columns = col_rename_dict, inplace=True)\n",
    "    \n",
    "    # convert percentage values to floats\n",
    "    for pctg_col in pctg_columns:\n",
    "        if pctg_col in df.columns:\n",
    "            df[pctg_col] = df[pctg_col].map(convert_percentage_float)\n",
    "            \n",
    "    df = df.rename(columns = lambda x: convert_camel_to_snake_case(x))\n",
    "                \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def base_read_clean_csv(path, fname, converters = {}, dtypes = {}, pctg_columns =  [], col_rename_dict = {}):\n",
    "    \"\"\"\n",
    "    Reads a data table and does some data cleaning and column name conversions and \n",
    "    returns a pandas dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): The path to the directory including the tables.\n",
    "        fname (str): The .csv (or other formats) filename.\n",
    "        converters (dict): A dictionary used for possible column value conversions.\n",
    "        dtypes (dict): A dictionary used for possible casting of column types.\n",
    "        pctg_columns (list): A list of that include column names with percentage values.\n",
    "        col_rename_dict (dict): A dictionary including the original column names \n",
    "        and their correpsonding new names.\n",
    "        \n",
    "    Returns:\n",
    "        df (DataFrame): The dataframe read and cleaned for the given file.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'{path}/{fname}', \n",
    "                        na_values=['*', 'N/A', '<NA>'],\n",
    "                         converters = converters,\n",
    "                         dtype = dtypes,\n",
    "                         keep_default_na = True,\n",
    "                       )\n",
    "\n",
    "    df = clean_data_frame(df, pctg_columns = pctg_columns, col_rename_dict = col_rename_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def read_data_write_clean_data(path=original_data_path, write_path=cleaned_data_path):\n",
    "    \"\"\"\n",
    "    Reads all the datasets in the form of DataFrame for all the \n",
    "    tables in the original data directory and conducts data cleaning on each\n",
    "    and saves them in the write_path.\n",
    "    It also combines all ACT and SAT scores for 3 years to their respective dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): The path to the directory of the original datasets.\n",
    "        write_path (str): The path to the directory of the clean datasets.\n",
    "        \n",
    "    Returns:\n",
    "        result (dict): The dictionary output of all the datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    pctg_columns = ['Participation', 'Accept Rate', 'Percent']\n",
    "    col_rename_dict = {'Evidence-Based Reading and Writing': 'EBRW',\n",
    "                         'Participation Rate': 'Participation'}\n",
    "    \n",
    "    file_names = get_list_of_csv_tables(path)\n",
    "    \n",
    "    for fname in file_names:\n",
    "        name = fname.split('.csv')[0]\n",
    "        converters = {}\n",
    "        \n",
    "        dtypes = {k: str for k in ['CDS', 'CCode', 'SCode', 'CDCode']} # for California data\n",
    "        \n",
    "        if name in ['act_2017']:\n",
    "            converters = {'Composite': lambda s: float(s.strip('x'))}\n",
    "            \n",
    "        if name.endswith('by_intended_college_major'):\n",
    "            name = name.replace('by_intended_college_major', 'by_major')\n",
    "            \n",
    "        df = base_read_clean_csv(path, fname, converters, dtypes, pctg_columns, col_rename_dict)\n",
    "        \n",
    "        years = get_year_from_str(name)\n",
    "        if years:\n",
    "            df['year'] = years[0]\n",
    "            \n",
    "        if name.endswith('_ca'):\n",
    "            df['state'] = 'California'\n",
    "            \n",
    "        out_file_name = f'{write_path}/{name}.csv'\n",
    "        \n",
    "        if name == 'sat_act_by_college':\n",
    "            df['school'].replace('--', ', ', inplace=True, regex=True)\n",
    "            df['school'].replace('—​', ', ', inplace=True, regex=True)\n",
    "        \n",
    "            # clean school names to be merged with rankings later\n",
    "            df['school'].replace({\n",
    "            'University of California, Los Angeles': 'University of California, Los Angeles (UCLA)',\n",
    "            'University of California, Berkeley': 'University of California Berkeley (UCB)',\n",
    "            'University of California, San Diego': 'University of California, San Diego (UCSD)',\n",
    "            'University of California, Davis': 'University of California, Davis (UCD)',\n",
    "            'University of California, Santa Cruz': 'University of California, Santa Cruz (UCSC)',\n",
    "            'University of California Riverside': 'University of California, Riverside (UCR)',\n",
    "            'University of California, Irvine': 'University of California, Irvine (UCI)',\n",
    "            'University of California, Santa Barbara': 'University of California, Santa Barbara (UCSB)', \n",
    "            'New York University': 'New York University (NYU)',\n",
    "            'Massachusetts Institute of Technology': 'Massachusetts Institute of Technology (MIT)',\n",
    "            'Georgia Institute of Technology': 'Georgia Institute of Technology (Georgia Tech)',\n",
    "            'California Institute of Technology': 'California Institute of Technology (Caltech)',\n",
    "            'New Jersey Institute of Technology': 'New Jersey Institute of Technology (NJIT)',\n",
    "            'University of Illinois, Chicago': 'University of Illinois, Chicago (UIC)',\n",
    "            'University at Albany, SUNY': 'University at Albany SUNY',\n",
    "            'Stony Brook University, SUNY': 'Stony Brook University SUNY',\n",
    "            'University at Buffalo, SUNY': 'University at Buffalo SUNY',\n",
    "            'University of Oklahoma': 'University of Oklahoma, Norman',\n",
    "            'University of Colorado, Boulder': 'University of Colorado Boulder',\n",
    "            'Virginia Polytechnic Institute and State University': 'Virginia Polytechnic Institute (Virginia Tech)',\n",
    "            'University of Texas, Austin': 'University of Texas at Austin',\n",
    "            'Texas A&M University, College Station': 'Texas A&M University',\n",
    "            'University of Texas, Dallas': 'University of Texas Dallas',\n",
    "            'University of Illinois, Urbana-​Champaign': 'University of Illinois at Urbana-Champaign',\n",
    "            'Rutgers The State University of New Jersey, Newark': 'Rutgers - The State University of New Jersey, Newark', \n",
    "            'Rutgers The State University of New Jersey, New Brunswick': 'Rutgers - The State University of New Jersey, New Brunswick', \n",
    "            'Purdue University, West Lafayette': 'Purdue University',\n",
    "            }, inplace=True)\n",
    "        \n",
    "        df.to_csv(out_file_name, index = False)\n",
    "        \n",
    "    sat_and_act = []\n",
    "    for test_type in ['sat', 'act']:\n",
    "        combined_data = pd.concat([pd.read_csv(f'{write_path}/{test_type}_{year}.csv') for year in [2017, 2018, 2019]])\n",
    "        sat_and_act.append(combined_data.set_index(['year', 'state']).add_prefix(f'{test_type}_').reset_index())\n",
    "        combined_data.to_csv(f'{write_path}/{test_type}.csv', index = False)\n",
    "        \n",
    "    sat_and_act_df = sat_and_act[0].merge(sat_and_act[1])\n",
    "\n",
    "    sat_and_act_df['sat_mandatory'] = False\n",
    "    sat_and_act_df.loc[sat_and_act_df['state'].isin(mandatory_sat_states), 'sat_mandatory']=True\n",
    "    sat_and_act_df['act_mandatory'] = False\n",
    "    sat_and_act_df.loc[sat_and_act_df['state'].isin(mandatory_act_states), 'act_mandatory']=True\n",
    "    \n",
    "    sat_and_act_df.to_csv(f'{write_path}/act_and_sat.csv', index = False)\n",
    "    \n",
    "def combine_sat_act_by_college_with_states(path=cleaned_data_path):\n",
    "    # import two dataframes to merge them\n",
    "    df1 = pd.read_csv(f'{path}/sat_act_by_college.csv'); # shape: (416, 8)\n",
    "    df2 = pd.read_csv(f'{path}/states_universities.csv');# (385, 2)\n",
    "    df3 = pd.read_csv(f'{path}/top_universities_rankings.csv'); \n",
    "    university_coordinates = pd.read_csv(f'{cleaned_data_path}/university_coordinates_states.csv')\n",
    "    \n",
    "    # merge two dataframes to add state as a column\n",
    "    df = df1.merge(df2, how='left', left_on='school', right_on='university').drop(columns='university'); # shape (417, 9)\n",
    "    \n",
    "    \n",
    "\n",
    "    df = df.merge(university_coordinates, right_on='school', left_on='school', how='left')\n",
    "    df['state_x'].fillna(df['state_y'], inplace=True)\n",
    "    df['state'] = df['state_x']\n",
    "    df['state'].fillna(df['school'].map(colleges_states), inplace=True)\n",
    "    \n",
    "    conv = {}\n",
    "    for state, abbrv in us_state_abbrev.items():\n",
    "        for school in df[df['state'].isnull()]['school']:\n",
    "            if state in school or f'{abbrv}' in school:\n",
    "                conv.update({school: state}) \n",
    "\n",
    "    df['state'].fillna(df['school'].map(conv), inplace=True)\n",
    "    \n",
    "    df = df.replace('--', '-')\n",
    "    df[['sat_25th_percentile', 'sat_75th_percentile']] = df['sat_total_25th-75th_percentile'].str.strip().str.split().str[-1].str.split('-', expand=True, n=1)\n",
    "    df[['act_25th_percentile', 'act_75th_percentile']] = df['act_total_25th-75th_percentile'].str.strip().str.split().str[-1].str.split('-', expand=True, n=1)\n",
    "    df.replace('', np.nan, inplace=True) \n",
    "    df = df.astype({c: float for c in ['act_25th_percentile', 'act_75th_percentile', 'sat_25th_percentile', 'sat_75th_percentile']})\n",
    "    df.drop(columns=['sat_total_25th-75th_percentile', 'act_total_25th-75th_percentile'], inplace=True)\n",
    "    df['sat_median'] = df[['sat_25th_percentile','sat_75th_percentile']].mean(axis = 1)\n",
    "    df['act_median'] = df[['act_25th_percentile','act_75th_percentile']].mean(axis = 1)\n",
    "    \n",
    "    df.dropna(subset=['sat_median'], inplace=True)\n",
    "    df['sat_median'] = df['sat_median'].astype(int)\n",
    "\n",
    "    \n",
    "    df['test_blind?'] = df['test_optional?']=='Yes (TB)'\n",
    "    df['test_free?'] = df['test_optional?']=='Yes (TF)'\n",
    "    df['test_optional?'].replace({k: 'Yes' for k in ['Yes*', 'Yes (TB)', 'Yes (TF)']}, inplace=True)\n",
    "    df['test_optional?'].replace({'Yes': True, 'No': False}, inplace = True)\n",
    "    \n",
    "    df['sat_mandatory'] = False\n",
    "    df.loc[df['state'].isin(mandatory_sat_states), 'sat_mandatory']=True\n",
    "    df['act_mandatory'] = False\n",
    "    df.loc[df['state'].isin(mandatory_act_states), 'act_mandatory']=True\n",
    "    \n",
    "    df = df.merge(df3, how='left', left_on='school', right_on='University').drop(columns='University'); \n",
    "\n",
    "    df['Rank'] = df['Rank'].astype('Int64')\n",
    "    df.rename(columns = {'sat_25th_percentile': 'sat_min', 'act_25th_percentile': 'act_min'}, inplace=True)\n",
    "              \n",
    "              \n",
    "    df = df[['school', 'state', 'number_of_applicants', 'accept_rate', 'sat_min', 'sat_median', 'act_min', 'act_median', 'test_optional?', \n",
    "             'Rank', 'coordinates', 'sat_mandatory', 'act_mandatory']]\n",
    "    \n",
    "    \n",
    "    df.to_csv(f'{path}/sat_act_by_college_in_states.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_high_ranking_colleges_with_coordinates():\n",
    "    \"\"\"\n",
    "    Returns a limited number of rows for the high ranking colleges.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'{cleaned_data_path}/sat_act_by_college_in_states.csv')\n",
    "    df = df[~df['Rank'].isnull()]\n",
    "    df.sort_values(by='Rank', ascending=True, inplace=True)\n",
    "    df['Rank'] = df['Rank'].astype(int)\n",
    "    df.set_index('Rank', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.to_csv(f'{cleaned_data_path}/high_ranking_colleges_with_coordinates.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_universities_required_scores_with_average_state_scores(path=cleaned_data_path):\n",
    "    #median_acceptance_score_by_average_of_universities_in_states \n",
    "    college_req = pd.read_csv(f'{path}/sat_act_by_college_in_states.csv')\n",
    "    college_req_scores = college_req.groupby('state')[['sat_min', 'sat_median', 'act_min', 'act_median']].mean().reset_index()\n",
    "\n",
    "    act_and_sat_all_years = pd.read_csv(f'{path}/act_and_sat.csv')\n",
    "    #act_and_sat = act_and_sat_all_years[act_and_sat_all_years['year']==2019][['sat_total', 'act_composite', 'state']]\n",
    "    act_and_sat = act_and_sat_all_years.set_index('year').groupby('state').max().reset_index()[['sat_total', 'act_composite', 'state']]\n",
    "\n",
    "    df = act_and_sat.merge(college_req_scores, on='state', how='left')\n",
    "    df['sat_mandatory'] = False\n",
    "    df.loc[df['state'].isin(mandatory_sat_states), 'sat_mandatory']=True\n",
    "    df['act_mandatory'] = False\n",
    "    df.loc[df['state'].isin(mandatory_act_states), 'act_mandatory']=True\n",
    "    df.rename(columns = {'act_composite': 'act_total'}, inplace=True)\n",
    "    df = df[['state', 'sat_min', 'sat_median', 'sat_total', 'act_min', 'act_median', 'act_total', 'sat_mandatory', 'act_mandatory']]\n",
    "    df.to_csv(f'{path}/req_scores_state_averages.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_data(path=cleaned_data_path):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of all the selected datasets.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): The path to the directory of the original datasets.\n",
    "        \n",
    "    Returns:\n",
    "        result (dict): The dictionary output of all the datasets.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    selected_datasets = ['sat_act_by_college_in_states.csv', 'sat_2019_by_major.csv', 'act_and_sat.csv', 'high_ranking_colleges_with_coordinates.csv', 'req_scores_state_averages.csv']\n",
    "    \n",
    "    for name in selected_datasets:\n",
    "        out.update({ name.split('.')[0] : pd.read_csv(f'{path}/{name}', dtype={'year': str, \n",
    "#                                                                                'Rank': 'Int64'\n",
    "                                                                              })})\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to run for generating the table for the coordinates of colleges/universities\n",
    "\n",
    "list_of_schools = pd.read_csv(f'{cleaned_data_path}/sat_act_by_college.csv')['school'].tolist()\n",
    "#get_coordinates_for_universities(list_of_schools)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Skidmore College, service\n",
    "University of California, San Diego (UCSD), emergency_ward_entrance\n",
    "Emerson College, education\n",
    "Pepperdine University, yes\n",
    "CUNY, Hunter, hamlet\n",
    "George Washington University, neighbourhood\n",
    "CUNY, Baruch College, hamlet\n",
    "University of Maryland, College Park, station\n",
    "University of Georgia, property\n",
    "Pennsylvania State University, University Park, post_office\n",
    "University of California, Santa Cruz (UCSC), garden\n",
    "Georgia Southern, peak\n",
    "San Jose State University, heritage\n",
    "New School, residential\n",
    "Georgia State, station\n",
    "University of Illinois at Urbana-Champaign, golf_course\n",
    "Yeshiva University, yes\n",
    "University at Buffalo SUNY, census\n",
    "Allegheny College, physiogeographical\n",
    "St. John Fisher College, census-designated\n",
    "Berry College, census\n",
    "California State University, San Bernardino, bus_stop\n",
    "California State University, Monterey Bay, bus_stop\n",
    "University of Colorado Boulder, administrative\n",
    "University of Tennessee, administrative\n",
    "Western Michigan, service\n",
    "Boise State, residential\n",
    "Siena College, census\n",
    "Illinois State, bus_stop\n",
    "Old Dominion, hamlet\n",
    "Portland State, administrative\n",
    "Western Kentucky, fast_food\n",
    "United States Air Force Academy, administrative\n",
    "United States Coast Guard Academy, attraction\n",
    "Louisiana State University, Baton Rouge, heritage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n",
      "['Kentucky - University of Kentucky', 'Louisiana - Tulane University', 'Maryland - Johns Hopkins University', 'Massachusetts - Massachusetts Institute of Technology (MIT)', 'Michigan - University of Michigan-Ann Arbor', 'Minnesota - University of Minnesota Twin Cities', 'Mississippi - Mississippi State University 1001-1200', 'Missouri - Washington University in St. Louis', 'Montana - University of Montana Missoula', 'Nebraska - University of Nebraska – Lincoln', 'New Hampshire:\\xa0Dartmouth College', 'New Jersey- Princeton University', 'New Mexico - University of New Mexico']\n",
      "[{'state': 'Kentucky', 'university': 'University of Kentucky'}, {'state': 'Louisiana', 'university': 'Tulane University'}, {'state': 'Maryland', 'university': 'Johns Hopkins University'}, {'state': 'Massachusetts', 'university': 'Massachusetts Institute of Technology (MIT)'}, {'state': 'Michigan', 'university': 'University of Michigan'}, {'state': 'Minnesota', 'university': 'University of Minnesota Twin Cities'}, {'state': 'Mississippi', 'university': 'Mississippi State University 1001'}, {'state': 'Missouri', 'university': 'Washington University in St. Louis'}, {'state': 'Montana', 'university': 'University of Montana Missoula'}, {'state': 'Nebraska', 'university': 'University of Nebraska – Lincoln'}, {'state': 'New Hampshire', 'university': 'Dartmouth College'}, {'state': 'New Jersey', 'university': 'Princeton University'}, {'state': 'New Mexico', 'university': 'University of New Mexico'}]\n"
     ]
    }
   ],
   "source": [
    "get_top_100_university_ratings_in_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data_write_clean_data()\n",
    "combine_sat_act_by_college_with_states()\n",
    "save_high_ranking_colleges_with_coordinates()\n",
    "combine_universities_required_scores_with_average_state_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
