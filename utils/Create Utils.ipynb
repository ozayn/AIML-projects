{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259613b5-6341-407e-8109-04f8fe6aeaef",
   "metadata": {},
   "source": [
    "### how to import\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# In notebook\n",
    "root_dir = Path().resolve().parents[1]  # Adjust if needed\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "from utils import general\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c5ea2c-8ab4-4e78-b1bb-ac5efab14cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting general.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile general.py\n",
    "\n",
    "import os\n",
    "import pkg_resources\n",
    "\n",
    "def run_command(command=\"\"):\n",
    "    os.system(command)\n",
    "\n",
    "def create_requirements_with_versions(req_name_orig=\"requirements0.txt\", \n",
    "                                      req_name=\"requirements.txt\", quiet=True):\n",
    "    install_command = f\"pip install -r {req_name_orig}\"\n",
    "    if quiet:\n",
    "        install_command+= \" -q\"\n",
    "    run_command(install_command)\n",
    "    \n",
    "    # Load the original package list (no versions)\n",
    "    with open(req_name_orig) as f:\n",
    "        wanted = {line.strip().lower() for line in f if line.strip() and not line.startswith(\"#\")}\n",
    "    \n",
    "    # Get installed distributions\n",
    "    installed = {dist.key: f\"{dist.project_name}=={dist.version}\" for dist in pkg_resources.working_set}\n",
    "    \n",
    "    # Match and save only the ones in your original list\n",
    "    with open(req_name, \"w\") as out:\n",
    "        for name in sorted(wanted):\n",
    "            match = installed.get(name.lower())\n",
    "            if match:\n",
    "                out.write(match + \"\\n\")\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "def is_running_colab():\n",
    "    return 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f143407e-bd6f-4195-b003-658ec5f1917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing misc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile misc.py\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_data(df):\n",
    "  cols_to_impute = df.columns[df.isnull().sum()>0].tolist()\n",
    "  cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "  df_imputed = df.copy()\n",
    "  for col in cols_to_impute:\n",
    "    strategy = 'most_frequent' if col in cat_cols else 'mean'\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    df_imputed[f'{col} (missing)'] = df_imputed[col]\n",
    "    df_imputed[[col]] = imputer.fit_transform(df_imputed[[f'{col} (missing)']])\n",
    "  return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ce5150-b91f-4566-924f-eef9a9cdeacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting eda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eda.py\n",
    "# Libraries to help with reading and manipulating data\n",
    "\n",
    "\n",
    "image_dir = '../images'\n",
    "\n",
    "import math\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import statsmodels.api as sm # for qqplot\n",
    "from scipy.stats import zscore # outliers\n",
    "import scipy.stats as stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# libaries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Library to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries to build decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# To tune different models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# To perform statistical analysis\n",
    "import scipy.stats as stats\n",
    "\n",
    "# To get diferent metric scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "# Library to suppress warnings or deprecation notes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def make_data_dictionary(df, ddict_str=\"\"):\n",
    "  dtypes = df.copy().dtypes\n",
    "  description_dict = {D[0]: ': '.join(D[1:]) for D in [d.split(\": \") for d in ddict_str.split('\\n')]}\n",
    "  dtypedf = pd.DataFrame(dtypes, columns=['Data Type'])\n",
    "  if ddict_str!='':\n",
    "    dtypedf['Description'] = dtypedf.index.map(description_dict)\n",
    "  else:\n",
    "    dtypedf['Description'] = dtypedf.index.to_series().apply(lambda x: x.replace('_', ' ').title())\n",
    "  # dtypedf['Description'] = dtypedf['Description'].fillna(dtypedf.Series(dtypedf.index, index=dtypedf.index))\n",
    "  dtypedf['# unique'] = df.nunique()\n",
    "  # if df.isnull().sum().sum()!=0:\n",
    "  #   dtypedf['# null'] = df.isnull().sum()\n",
    "  dtypedf.index.name='Column'\n",
    "  return dtypedf\n",
    "\n",
    "\n",
    "def get_anomalous_values(df, positive_cols=[]):\n",
    "  L = ['Anamolous Values']\n",
    "  data_num = df.select_dtypes(include=\"number\")\n",
    "  count_neg = (data_num < 0).sum()\n",
    "  neg_dict_count = count_neg[count_neg>0].to_dict()\n",
    "  for f, count in neg_dict_count.items():\n",
    "    L.append(f'There are {count:,} negative values in {f} column.')\n",
    "  return L\n",
    "\n",
    "def feature_engineering(df, feature_info={}):\n",
    "  \"\"\"\n",
    "  feature_info = {'formula': {'ZIPCode': ('SCF', lambda x: x[0:2])},\n",
    "                'dtype': {'SCF': 'category'},\n",
    "                'description': {'SCF': '(sectional center facility) is the first two digits of the ZIPCode.'}\n",
    "                }\n",
    "  \"\"\"\n",
    "  if feature_info == {}:\n",
    "    return df, 'No feature engineering is done.'\n",
    "  formula = feature_info['formula']\n",
    "  dtype = feature_info['dtype']\n",
    "  description = feature_info['description']\n",
    "  print(description)\n",
    "\n",
    "  L = []\n",
    "  for originalfield, v in formula.items():\n",
    "    df[v[0]] = df[originalfield].apply(v[1])\n",
    "    if v[0]!=originalfield:\n",
    "      L.append(f'The {v[0]} column is created from the {originalfield} column.')\n",
    "    else:\n",
    "      L.append(f'The {originalfield} column is modified.')\n",
    "  for f, t in dtype.items():\n",
    "    df[f] = df[f].astype(t)\n",
    "  for f, desc in description.items():\n",
    "    L.append(f'{f}: {desc}')\n",
    "\n",
    "  t = '\\n'.join(L)\n",
    "  return df, t\n",
    "\n",
    "\n",
    "def data_preprocessing(df, positive_cols=[], drop_cols = [], type_conv={},\n",
    "                        feature_info={}):\n",
    "  df, L = correct_anomalous_values(df, positive_cols=positive_cols)\n",
    "  df, t_type_conv = convert_dtypes(df, type_conv=type_conv)\n",
    "  df, t_feature = feature_engineering(df, feature_info=feature_info)\n",
    "  L.append(t_feature)\n",
    "  if drop_cols:\n",
    "    df = df.drop(drop_cols, axis = 1)\n",
    "    L.append(f\"Column(s) {', '.join(drop_cols)} is/are dropped.\")\n",
    "\n",
    "\n",
    "  L.append(t_type_conv)\n",
    "\n",
    "  text = '\\n\\n'.join(L)\n",
    "  return df, text\n",
    "\n",
    "\n",
    "def correct_anomalous_values(df, positive_cols=[]):\n",
    "  data_num = df.select_dtypes(include=\"number\")\n",
    "  count_neg = (data_num < 0).sum()\n",
    "  L = []\n",
    "  neg_dict_count = count_neg[count_neg>0].to_dict()\n",
    "  for f, count in neg_dict_count.items():\n",
    "    if f in positive_cols:\n",
    "      df[f] = df[f].map(lambda x: abs(x))\n",
    "      L.append(f'{count:,} values in {f} column are converted to positive values.')\n",
    "  return df, L\n",
    "\n",
    "def show_null_columns(df):\n",
    "  if df.isnull().sum().sum()==0:\n",
    "    return pd.DataFrame()\n",
    "  # nc = df.isnull().sum().sort_values(ascending=False).to_frame('# Null')\n",
    "  nc = df.isnull().sum()[df.isnull().sum()!=0].sort_values(ascending=False).to_frame('# Null')\n",
    "  return nc\n",
    "\n",
    "def get_memory_usage(df):\n",
    "  return df.memory_usage(deep=False).sum()/1024\n",
    "\n",
    "def get_summary_info(df):\n",
    "  out = {}\n",
    "  out['Memory Usage'] = f\"{get_memory_usage(df): .1f} KB\"\n",
    "  out[''] = ''\n",
    "  out['#'] = ''\n",
    "  out['Rows'] = df.shape[0]\n",
    "  out['Columns'] = df.shape[1]\n",
    "  out['Null Values'] = df.isnull().sum().sum()\n",
    "  out['Duplicated Rows'] = df.duplicated().sum()\n",
    "  out[' '] = ' '\n",
    "  p1 = pd.DataFrame(out, index=['']).T\n",
    "  p2 = df.dtypes.astype(str).value_counts().to_frame('')\n",
    "  p = pd.concat([p1, p2])\n",
    "  return p\n",
    "\n",
    "\n",
    "def get_describe_tables(df):\n",
    "  data_obj_cat = df.select_dtypes(include=['object', 'category'])\n",
    "  data_num = df.select_dtypes(include='number')\n",
    "\n",
    "  descr_o_cat = pd.DataFrame()\n",
    "  descr_n = pd.DataFrame()\n",
    "\n",
    "  number_df = df.select_dtypes(include=['number'])\n",
    "  if data_obj_cat.shape[1]> 0:\n",
    "    descr_o_cat = df.describe(include=['object', 'category']).T\n",
    "    descr_o_cat.drop(['count'], axis=1, inplace=True)\n",
    "    descr_o_cat.index.name = 'Object/Categorical Column'\n",
    "\n",
    "  # if data_num.shape[1]> 0:\n",
    "  #   descr_n = df.describe(include='number').T\n",
    "  #   descr_n['count']= descr_n['count'].astype('int')\n",
    "  #   descr_n['IQR'] = descr_n['75%'] - descr_n['25%']\n",
    "  #   descr_n['Lower Bound'] = descr_n['25%'] - 1.5 * descr_n['IQR']\n",
    "  #   descr_n['Upper Bound'] = descr_n['75%'] + 1.5 * descr_n['IQR']\n",
    "  #   descr_n['Outlier count (Upper)'] = (data_num >= descr_n['Upper Bound']).sum()\n",
    "  #   descr_n['Outlier count (Lower)'] = (data_num <= descr_n['Lower Bound']).sum()\n",
    "  #   descr_n['Outlier count'] = descr_n['Outlier count (Upper)'] + descr_n['Outlier count (Lower)']\n",
    "  #   descr_n.drop(['count'], axis=1, inplace=True)\n",
    "  #   descr_n.index.name = 'Numerical Column'\n",
    "\n",
    "  if data_num.shape[1]> 0:\n",
    "    descr_n = df.describe(include='number').T\n",
    "    descr_n['mean'] = descr_n['mean'].round(1)\n",
    "    descr_n['std'] = descr_n['std'].round(1)\n",
    "    descr_n['count']= descr_n['count'].astype('int')\n",
    "    descr_n['IQR'] = descr_n['75%'] - descr_n['25%']\n",
    "    descr_n['Lower Bound'] = descr_n['25%'] - 1.5 * descr_n['IQR']\n",
    "    descr_n['Upper Bound'] = descr_n['75%'] + 1.5 * descr_n['IQR']\n",
    "    descr_n['# Outliers (Upper)'] = (data_num >= descr_n['Upper Bound']).sum()\n",
    "    descr_n['# Outliers (Lower)'] = (data_num <= descr_n['Lower Bound']).sum()\n",
    "    descr_n['# Outliers'] = descr_n['# Outliers (Upper)'] + descr_n['# Outliers (Lower)']\n",
    "    descr_n['Outliers %'] = (100 * descr_n['# Outliers']/data_num.shape[0]).round(1)\n",
    "    descr_n.drop(['count', 'Upper Bound', 'Lower Bound'], axis=1, inplace=True)\n",
    "    descr_n.index.name = 'Numerical Column'\n",
    "  return [descr_o_cat, descr_n]\n",
    "\n",
    "\n",
    "def round_num(n):\n",
    "  if n > 1e6:\n",
    "    return f' (~{n/ 1e6:.0f}M)'\n",
    "  elif n>1e3:\n",
    "    return f' (~{n/ 1e3:.0f}K)'\n",
    "  else:\n",
    "    return ''\n",
    "\n",
    "\n",
    "def null_text_info(df):\n",
    "  L = []\n",
    "  n = df.isnull().sum().sum()\n",
    "  if n > 0:\n",
    "    null_pct = (df.isnull().sum()/df.shape[0]*100).sort_values(ascending=False)\n",
    "    return [f\"There are _{n:,} null values_ in the dataset. {null_pct.iloc[0]}% of the missing data is in {null_pct.index[0]}.\"]\n",
    "  return ['There are __no missing__ values in the data.']\n",
    "\n",
    "\n",
    "def convert_dtypes(df, type_conv=None):\n",
    "  t = ''\n",
    "  if type_conv!= {}:\n",
    "    if type(list(type_conv.values())[0])==list:\n",
    "      dtypes = {col: k for k, v in type_conv.items() for col in v}\n",
    "      df = df.astype(dtypes)\n",
    "      t = '\\n'.join([f\"{', '.join(v)} are/is converted to {k} type.\" for k, v in type_conv.items()])\n",
    "    else:\n",
    "      df = df.astype(type_conv)\n",
    "      for k, v in type_conv.items():\n",
    "        t += f\"'{k}' is converted to {v} type.\\n\"\n",
    "  return df, t\n",
    "\n",
    "\n",
    "def duplicated_text_info(df):\n",
    "  n = df.duplicated().sum()\n",
    "  if n > 0:\n",
    "    return [f'There are _{n:,} duplicated rows_ in the dataset.']\n",
    "  return ['There are __no duplicated__ rows in the data.']\n",
    "\n",
    "def get_text_info(df):\n",
    "  L = []\n",
    "  L.append(f'There are {df.shape[0]:,}{round_num(df.shape[0])} _rows_ and {df.shape[1]} _columns_ in the dataset.')\n",
    "  L.append(f\"The __memory usage__ is approximately {get_memory_usage(df): .1f} KB.\")\n",
    "  L.extend(null_text_info(df))\n",
    "  L.extend(duplicated_text_info(df))\n",
    "  L.extend(get_anomalous_values(df))\n",
    "  return '\\n\\n'.join(L)\n",
    "\n",
    "\n",
    "def recommendations(data, ddict_str):\n",
    "  recommend_category = data.columns[data.select_dtypes(exclude=['category']).nunique()<=10].tolist()\n",
    "\n",
    "  print('Recommendations:\\n'+'-'*120)\n",
    "  print('Convert to category: ')\n",
    "  print('type_conv =')\n",
    "  pprint({r: 'category' for r in recommend_category})\n",
    "\n",
    "  cat_orders = {}\n",
    "  convert_to_int = lambda x: int(x) if type(x) == str and x.isdigit() else x\n",
    "  import re\n",
    "  pattern = r'\\b\\d+-[A-Za-z]+\\b'\n",
    "  ddict = {k: v for k, v in [a.split(\": \") for a in ddict_str.split('\\n')]}\n",
    "  for k, v in ddict.items():\n",
    "    if v.count('-') > 1:\n",
    "      matches = re.findall(pattern, v)\n",
    "      values = [convert_to_int(m.split('-')[0].strip()) for m in matches]\n",
    "      cat_orders[k] = values\n",
    "  print('cat_orders = ')\n",
    "  pprint(cat_orders)\n",
    "\n",
    "  single_value_columns = data.nunique()[data.nunique()==1].index.tolist()\n",
    "  possible_id_columns = data.nunique()[data.nunique()==data.shape[0]].index.tolist()\n",
    "  print('drop_cols =', single_value_columns,'+', possible_id_columns)\n",
    "  print('-'*120)\n",
    "\n",
    "\n",
    "def modify_data(data, modify_dict ={}):\n",
    "  if modify_dict!={}:\n",
    "\n",
    "    for field, conv in modify_dict.items():\n",
    "      data[field].replace(conv[0], conv[1], inplace=True)\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "\n",
    "def create_info(data, positive_cols=[], drop_cols=[], type_conv={},\n",
    "                  feature_info={}, modify_dict={}, ddict_str=\"\", cat_orders={}):\n",
    "  mkdown_name = \"info_dataframes.md\"\n",
    "  recommendations(data, ddict_str)\n",
    "  modify_data(data, modify_dict = modify_dict)\n",
    "  text = get_text_info(data)\n",
    "  mkdf = []\n",
    "  mkdf.append(make_data_dictionary(data, ddict_str))\n",
    "  mkdf.append(get_summary_info(data))\n",
    "  mkdf.append(show_null_columns(data))\n",
    "  data, t_preprocessing = data_preprocessing(data, positive_cols=positive_cols,\n",
    "                                            type_conv=type_conv,\n",
    "                                             feature_info=feature_info,\n",
    "                                             drop_cols = drop_cols)\n",
    "  for k, v in cat_orders.items():\n",
    "    data[k] = pd.Categorical(data[k], categories=v, ordered=True)\n",
    "  mkdf.append(make_data_dictionary(data, ddict_str))\n",
    "  mkdf.append(get_summary_info(data))\n",
    "  mkdf.append(show_null_columns(data))\n",
    "  text += '\\n\\n' + t_preprocessing\n",
    "  print(text)\n",
    "  mkdf.extend(get_describe_tables(data))\n",
    "  with open(mkdown_name, \"w\") as f:\n",
    "    f.write(text)\n",
    "    f.write(\"\\n\\n\")\n",
    "    for i, d in enumerate(mkdf, start=1):\n",
    "      display(d)\n",
    "      f.write(d.to_markdown())\n",
    "      f.write(\"\\n\\n\")\n",
    "  print(f'Information is written to {mkdown_name}')\n",
    "  return data\n",
    "\n",
    "\n",
    "\n",
    "def is_categorical_data_uniform(data, name = None, alpha = .05):\n",
    "    \"\"\"\n",
    "    Analyzes the distribution of categorical data to see if it resembles\n",
    "    a uniform distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    if name is None:\n",
    "        name = data.name\n",
    "\n",
    "    # Frequency distribution\n",
    "    counts = data.value_counts().to_frame()\n",
    "    counts.columns=['observed']\n",
    "    counts.index.name = 'categories'\n",
    "    counts['expected'] = counts['observed'].mean()\n",
    "\n",
    "    # Perform Chi-Square Goodness-of-Fit Test\n",
    "    chi2_stat, p_value = chisquare(counts['observed'], counts['expected'])\n",
    "\n",
    "\n",
    "    H0 = f\"'{name}' APPEARS to follow a balanced/uniform-like distribution (p: {p_value:.3f}).\"\n",
    "    Ha = f\"'{name}' does NOT follow a balanced/uniform-like distribution (p: {p_value:.3f}).\"\n",
    "\n",
    "    is_balanced = True\n",
    "    H = H0\n",
    "\n",
    "    if p_value < alpha:\n",
    "        is_balanced = False\n",
    "        H = Ha\n",
    "\n",
    "    return is_balanced, H\n",
    "\n",
    "\n",
    "def categorical_countplot(data, figsize = None, palette = 'Paired',\n",
    "                          perc = True, rotation=0, nmax = None, to_sort = False,\n",
    "                          flip_axes = False):\n",
    "\n",
    "  thresh = 9\n",
    "  get_top = False\n",
    "\n",
    "  name = data.name\n",
    "  if nmax == None:\n",
    "    nmax = data.nunique()\n",
    "\n",
    "  if nmax > thresh:\n",
    "    flip_axes = True\n",
    "\n",
    "\n",
    "  if nmax < data.nunique():\n",
    "    get_top = True\n",
    "\n",
    "  if not flip_axes:\n",
    "    if type(data.unique()[0]) == str:\n",
    "      if rotation == 0:\n",
    "        rotation = 60\n",
    "\n",
    "  total = len(data)\n",
    "\n",
    "  if figsize == None:\n",
    "    figsize = (min(nmax+1, 10), 3)\n",
    "    if flip_axes:\n",
    "      figize = figsize = (4, min(nmax-1, 10))\n",
    "\n",
    "  plt.figure(figsize=figsize)\n",
    "\n",
    "  name = modify_varname(name)\n",
    "\n",
    "\n",
    "  order = None\n",
    "  if to_sort or get_top:\n",
    "    order = data.value_counts().index[:nmax]\n",
    "    if get_top:\n",
    "      data = data[data.isin(order)]\n",
    "\n",
    "\n",
    "  if flip_axes:\n",
    "    ax = sns.countplot(\n",
    "        y=data,\n",
    "        hue=data,\n",
    "        palette=palette,\n",
    "        order=order,\n",
    "        legend=False\n",
    "    )\n",
    "  else:\n",
    "    ax = sns.countplot(\n",
    "      x=data,\n",
    "      hue=data,\n",
    "      palette=palette,\n",
    "      order=order,\n",
    "      legend=False\n",
    "    )\n",
    "\n",
    "  if flip_axes:\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(100 * p.get_width() / total)\n",
    "        else:\n",
    "            label = p.get_width()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width()\n",
    "        y = p.get_y() + p.get_height()/2  # height of the plot\n",
    "\n",
    "        xytext=(30, 0)\n",
    "\n",
    "        # annotate the percentage\n",
    "        ax.annotate(label,(x, y),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    xytext=xytext,\n",
    "                    textcoords=\"offset points\")\n",
    "    plt.ylabel(name)\n",
    "  else:\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(100 * p.get_height() / total)\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        xytext=(0, 11)\n",
    "\n",
    "        # annotate the percentage\n",
    "        ax.annotate(label,(x, y),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    xytext=xytext,\n",
    "                    textcoords=\"offset points\")\n",
    "    plt.xlabel(name)\n",
    "    plt.xticks(rotation=rotation)\n",
    "\n",
    "\n",
    "  title = f'Count Plot of {name}'\n",
    "  if get_top:\n",
    "    title += f' (Top {nmax})'\n",
    "  plt.title(title, pad = 25)\n",
    "  sns.despine(ax=ax)\n",
    "  plt.savefig(f\"{image_dir}/countplot_{name}.png\", bbox_inches='tight')\n",
    "\n",
    "def univariate_categorical_info(data):\n",
    "  print(f'\\n{data.name}')\n",
    "  n = data.nunique()\n",
    "  values = data.unique().tolist()\n",
    "  proportions = 100 * data.value_counts(normalize=True)\n",
    "  majority_index = proportions.index[0]\n",
    "  majority_proportion = proportions.iloc[0]\n",
    "  if n == 2:\n",
    "    if majority_index in [1, 'Yes', 'yes', True]:\n",
    "      return f'The majority are {data.name} ({majority_proportion:.1f}%).'\n",
    "    elif majority_index in [0, 'No', 'no', False]:\n",
    "      return f'The majority are not {data.name} ({majority_proportion:.1f}%).'\n",
    "  if n > 3:\n",
    "    return f'The most common value in {data.name} is {majority_index} ({majority_proportion:.1f}%) followed by {proportions.index[1]} ({proportions.iloc[1]:.1f}%).'\n",
    "\n",
    "\n",
    "  return f'The most common value in {data.name} is {majority_index} ({majority_proportion:.1f}%).'\n",
    "\n",
    "\n",
    "def univariate_categorical(data, name = None, figsize = None, palette = 'Paired',\n",
    "                          perc = True, rotation=0, nmax = None):\n",
    "    \"\"\"\n",
    "    Analyzes the distribution of categorical data to see if it resembles\n",
    "    a balanced or uniform-like distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    if name is None:\n",
    "        name = data.name\n",
    "\n",
    "    # print(f\"\\nAnalyzing categorical data: {name}\")\n",
    "\n",
    "    is_balanced, H = is_categorical_data_uniform(data)\n",
    "    if is_balanced:\n",
    "      print(H)\n",
    "\n",
    "    categorical_countplot(data, figsize = figsize, palette = palette,\n",
    "                          perc = perc, rotation=rotation, nmax = nmax)\n",
    "\n",
    "    print(univariate_categorical_info(data))\n",
    "\n",
    "\n",
    "def modify_varname(name):\n",
    "  name = name.replace('_', ' ')\n",
    "  conv = {' Amt': ' Amount',\n",
    "          ' Chng': ' Change',\n",
    "          ' Bal': ' Balance',\n",
    "          ' mon': ' Months',\n",
    "          ' Trans': ' Transaction',\n",
    "          'Q4 Q1': 'Q4-Q1',\n",
    "          ' To ': ' to ',\n",
    "          ' Ct': ' Count'}\n",
    "  for k, v in conv.items():\n",
    "    if k in name:\n",
    "      name = name.replace(k, v)\n",
    "  return name\n",
    "\n",
    "def histplot_boxplot(data, figsize = (9, 5), height_ratios = [3, 1],\n",
    "             round_by=2,\n",
    "             kde = True,\n",
    "             mean_color='blue',\n",
    "             median_color='gray',\n",
    "             color='skyblue',\n",
    "             alpha_hist = .3,\n",
    "             alpha_box = .3,\n",
    "             showmeans=True):\n",
    "  name = data.name\n",
    "\n",
    "  name = modify_varname(name)\n",
    "\n",
    "  # Create subplots with shared x-axis\n",
    "  fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True,\n",
    "                           figsize=figsize,\n",
    "                           gridspec_kw={'height_ratios': height_ratios })\n",
    "\n",
    "  # Plot the histogram\n",
    "  sns.histplot(data, kde=kde, ax=axes[0], color=color,\n",
    "               alpha=alpha_hist,\n",
    "               stat = 'density',\n",
    "               edgecolor='black', linewidth=.5)\n",
    "  axes[0].set_title(f\"{name} - Histogram & Boxplot\")\n",
    "  axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "  # Plot the boxplot\n",
    "  sns.boxplot(x=data, ax=axes[1], color=color, width=0.5,\n",
    "              showmeans = showmeans,\n",
    "              boxprops=dict(alpha=alpha_box),\n",
    "              meanprops={\"marker\": \"^\", \"markerfacecolor\": mean_color, \"markeredgecolor\": \"black\"},\n",
    "              flierprops={\"marker\": \"x\", \"markersize\": 3, \"markerfacecolor\": \"gray\"})\n",
    "  axes[1].set_xlabel(name)\n",
    "\n",
    "  sns.despine(ax=axes[0])\n",
    "  sns.despine(ax=axes[1])\n",
    "\n",
    "\n",
    "  if showmeans:\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "\n",
    "    # Plot mean and median lines\n",
    "    axes[0].axvline(x = mean, color=mean_color, linestyle='--', linewidth=.5, label=f'Mean: {round(mean, round_by):,}')\n",
    "    axes[0].axvline(x = median, color=median_color, linestyle='-', linewidth=.5, label=f'Median: {round(median, round_by):,}')\n",
    "    axes[0].legend(frameon=False, loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "\n",
    "\n",
    "  # Adjust layout and show\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f\"{image_dir}/histplot_boxplot_{name}.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "def qqplot(data, color='skyblue', figsize=(4,3.50)):\n",
    "\n",
    "  name = data.name\n",
    "\n",
    "  name = modify_varname(name)\n",
    "\n",
    "  data_cleaned = data[~np.isnan(data)]\n",
    "\n",
    "  plt.figure(figsize=figsize)\n",
    "  # Create Q-Q plot\n",
    "  fig = sm.qqplot(data_cleaned, line='s', ax=plt.gca());\n",
    "\n",
    "  # Customize the color of the points\n",
    "  plt.gca().get_lines()[1].set_color(color)  # Set points to purple\n",
    "\n",
    "\n",
    "  plt.title(f\"Q-Q Plot of {name}\")\n",
    "  sns.despine(top=True, right=True)\n",
    "  plt.tight_layout();\n",
    "  plt.savefig(f\"{image_dir}/qqplot_{name}.png\", bbox_inches='tight')\n",
    "\n",
    "def is_numerical_data_normal(data):\n",
    "  name = data.name\n",
    "\n",
    "  from scipy.stats import skew, kurtosis\n",
    "\n",
    "  data_skewness = skew(data)\n",
    "  data_kurtosis = kurtosis(data, fisher=False)  # Fisher=False returns kurtosis with normal = 3\n",
    "\n",
    "  # Calculate skewness and specify its level and direction\n",
    "  if data_skewness > 0:\n",
    "      if data_skewness < 0.5:\n",
    "          skew_description = \"slightly positively skewed (right-skewed)\"\n",
    "      else:\n",
    "          skew_description = \"highly positively skewed (right-skewed)\"\n",
    "  elif data_skewness < 0:\n",
    "      if data_skewness > -0.5:\n",
    "          skew_description = \"slightly negatively skewed (left-skewed)\"\n",
    "      else:\n",
    "          skew_description = \"highly negatively skewed (left-skewed)\"\n",
    "  else:\n",
    "      skew_description = \"approximately symmetric\"\n",
    "\n",
    "  # print(f\"{name} - Skewness: {data_skewness:.4f} ({skew_description})\")\n",
    "  # print(f\"{name} - Kurtosis: {data_kurtosis:.4f}\")\n",
    "\n",
    "  t = f'{name} is {skew_description} (skewness: {data_skewness: .2f}) with average of {data.mean():.2f}.'\n",
    "  print(t)\n",
    "\n",
    "  if abs(data_skewness) < 0.5 and abs(data_kurtosis - 3) < 0.5:\n",
    "    print(f\"The data {name} is LIKELY CLOSE to normal distribution.\")\n",
    "    return True\n",
    "  else:\n",
    "    print(f\"The data {name} may NOT be normally distributed.\")\n",
    "    return False\n",
    "\n",
    "def get_outlier_percentage(data):\n",
    "  name = data.name\n",
    "\n",
    "  # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "  Q1 = data.quantile(0.25)\n",
    "  Q3 = data.quantile(0.75)\n",
    "  IQR = Q3 - Q1\n",
    "\n",
    "  # Define outlier thresholds\n",
    "  lower_bound = Q1 - 1.5 * IQR\n",
    "  upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "  # Identify outliers\n",
    "  outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "  # Calculate percentage of outliers\n",
    "  percentage_outliers = (len(outliers) / len(data)) * 100\n",
    "  print(f\"Percentage of {name} outliers using IQR: {percentage_outliers:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Calculate z-scores\n",
    "  dfzscore = zscore(data)\n",
    "\n",
    "  # Define outliers (e.g., z-score > 3 or < -3)\n",
    "  outliers_z = data[(dfzscore > 3) | (dfzscore < -3)]\n",
    "\n",
    "  # Calculate percentage of outliers\n",
    "  percentage_outliers_z = (len(outliers_z) / len(data)) * 100\n",
    "  print(f\"Percentage of {name} outliers using Z-score: {percentage_outliers_z:.2f}%\")\n",
    "\n",
    "\n",
    "def univariate_numerical(data, color, proportion_thresh = .6):\n",
    "    print(f'\\n{data.name}')\n",
    "    is_normal = is_numerical_data_normal(data)\n",
    "    get_outlier_percentage(data)\n",
    "    histplot_boxplot(data, color=color, kde=is_normal)\n",
    "    qqplot(data, color=color)\n",
    "\n",
    "def pairplot(data, vars = [], hue = None, palette = None,\n",
    "             marker = 'o', alpha = 0.5,\n",
    "             figsize = None,\n",
    "             rotation = 0):\n",
    "  if figsize:\n",
    "    plt.figure(figsize=figsize)\n",
    "  else:\n",
    "    plt.figure()\n",
    "\n",
    "  if vars == []:\n",
    "    vars = data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "  g = sns.pairplot(data, vars = vars,\n",
    "            corner = True,\n",
    "            hue = hue,\n",
    "            palette = palette,\n",
    "            plot_kws=dict(alpha=alpha, marker=marker, linewidth=1),\n",
    "            diag_kws=dict(fill=False),\n",
    "            );\n",
    "\n",
    "  for ax in g.axes.flatten():\n",
    "    if ax:\n",
    "        # rotate x axis labels\n",
    "        ax.set_xlabel(ax.get_xlabel(), rotation = rotation);\n",
    "        # rotate y axis labels\n",
    "        ax.set_ylabel(ax.get_ylabel(), rotation = 0);\n",
    "        # set y labels alignment\n",
    "        ax.yaxis.get_label().set_horizontalalignment('right');\n",
    "\n",
    "  fname = 'pairplot'\n",
    "  if hue:\n",
    "    fname += f'_hue-{hue}'\n",
    "\n",
    "  if hue:\n",
    "\n",
    "    hue_name = hue.replace('_', '\\n')\n",
    "\n",
    "\n",
    "    sns.move_legend(g, \"upper right\", bbox_to_anchor=(0.8, 0.95), title = hue_name)\n",
    "\n",
    "  plt.savefig(f\"{image_dir}/{fname}.png\", bbox_inches='tight');\n",
    "\n",
    "\n",
    "def correlation_numeric_analysis(corr):\n",
    "  upper_triangle_indices = np.triu_indices_from(corr, k=1)\n",
    "  corr_pairs = [\n",
    "      (corr.index[i], corr.columns[j], corr.iloc[i, j])\n",
    "      for i, j in zip(*upper_triangle_indices)\n",
    "  ]\n",
    "\n",
    "  # Convert to DataFrame for sorting\n",
    "  corr_pairs_df = pd.DataFrame(corr_pairs, columns=[\"Variable 1\", \"Variable 2\", \"Correlation\"])\n",
    "\n",
    "  # Sort the pairs by correlation value\n",
    "  sorted_corr_pairs = corr_pairs_df.sort_values(by=\"Correlation\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  return sorted_corr_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def heatmap(matrix,figsize=(15, 7), vmin = -1 , vmax = 1, corner = True,\n",
    "            rotation_x = 45, rotation_y = 45, annot=True, to_rename = True,\n",
    "            tick_fontsize = None,\n",
    "            annot_fontsize = 12,\n",
    "            name=''):\n",
    "\n",
    "  if to_rename:\n",
    "    matrix_columns = matrix.columns.tolist()\n",
    "    matrix_index = matrix.index.tolist()\n",
    "\n",
    "    #rename_var = lambda x: x.replace('_', '\\n')\n",
    "    # rename_var = lambda x: x.replace('_', ' ')\n",
    "    rename_var = modify_varname\n",
    "\n",
    "    matrix.columns = list(map(rename_var, matrix_columns))\n",
    "    matrix.index = list(map(rename_var, matrix_index))\n",
    "\n",
    "\n",
    "  mask = None\n",
    "  plt.figure(figsize = figsize)\n",
    "\n",
    "  shrink = 1\n",
    "\n",
    "  corner_or_square = 'corner' if corner else 'square'\n",
    "\n",
    "  if corner:\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "    matrix_rm_diag = matrix.where(np.triu(np.ones(matrix.shape), k=1).astype(bool))\n",
    "\n",
    "\n",
    "    round_num_up = lambda x: -math.floor(abs(x) * 10) / (10) if x<0 else math.ceil(x * 10) / (10)\n",
    "    round_num_down = lambda x: -math.ceil(abs(x) * 10) / (10) if x<0 else math.floor(x * 10) / (10)\n",
    "    min_value = matrix_rm_diag.min().min()\n",
    "    max_value = matrix_rm_diag.max().max()\n",
    "\n",
    "    vmin = round_num_down(min_value)\n",
    "    vmax = round_num_up(max_value)\n",
    "\n",
    "    max_abs = max(abs(vmin), abs(vmax))\n",
    "    vmin = -max_abs\n",
    "    vmax = max_abs\n",
    "\n",
    "  if corner:\n",
    "    shrink = (matrix.shape[0]-2)/matrix.shape[0]\n",
    "\n",
    "\n",
    "  ax = sns.heatmap(matrix, mask=mask, annot=annot, vmin=vmin, vmax=vmax, fmt=\".2f\",\n",
    "              cbar_kws={\"shrink\": shrink, 'pad': 0},\n",
    "              annot_kws={\"size\": annot_fontsize},\n",
    "              linewidth=2,\n",
    "              square = True,\n",
    "              cmap=\"coolwarm\")\n",
    "\n",
    "  ax.set(xlabel=\"\", ylabel=\"\")\n",
    "  # ax.xaxis.tick_top()\n",
    "  if tick_fontsize:\n",
    "    ax.tick_params(axis='both', labelsize=tick_fontsize)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation_x, ha='right')\n",
    "  ax.set_yticklabels(ax.get_yticklabels(), rotation=rotation_y)\n",
    "\n",
    "  plt.tick_params(left=False, bottom=False)\n",
    "\n",
    "\n",
    "  plt.savefig(f\"{image_dir}/heatmap_{name}_{corner_or_square}.png\", bbox_inches='tight');\n",
    "\n",
    "\n",
    "def bivariate_numerical(df, numerical_cols, figsize=(4,4), color = 'skyblue',\n",
    "                        alpha = .3):\n",
    "\n",
    "\n",
    "\n",
    "  if len(numerical_cols)>1:\n",
    "    pairplot(df, figsize=(13, 13), rotation=30)\n",
    "    # for col in categorical_cols:\n",
    "    #   if df[col].nunique() < 5:\n",
    "    #     pairplot(df, hue = col, palette = palette_dict_cat[col])\n",
    "    corr = df.corr(numeric_only=True)\n",
    "    table = correlation_numeric_analysis(corr)\n",
    "    display(table)\n",
    "    heatmap(corr,figsize=(18, 10), corner = True, name='corr_numeric',\n",
    "            rotation_y=0, rotation_x=30, tick_fontsize=12, annot_fontsize=12)\n",
    "    heatmap(corr,figsize=(18, 10), corner = False, name='corr_numeric',\n",
    "            rotation_y=0, rotation_x=30, tick_fontsize=12, annot_fontsize=12)\n",
    "\n",
    "\n",
    "def conditional_entropy(x, y):\n",
    "    \"\"\"Calculate the conditional entropy of x given y.\"\"\"\n",
    "    y_counter = Counter(y)\n",
    "    xy_counter = Counter(list(zip(x, y)))\n",
    "    total_occurrences = sum(y_counter.values())\n",
    "    entropy = 0.0\n",
    "    for (x_val, y_val), xy_count in xy_counter.items():\n",
    "        p_xy = xy_count / total_occurrences\n",
    "        p_y = y_counter[y_val] / total_occurrences\n",
    "        entropy += p_xy * np.log(p_y / p_xy)\n",
    "    return entropy\n",
    "\n",
    "def theils_u(x, y):\n",
    "    \"\"\"Calculate Theil's U (Uncertainty Coefficient).\"\"\"\n",
    "    s_xy = conditional_entropy(x, y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    s_x = -sum((count / total_occurrences) * np.log(count / total_occurrences) for count in x_counter.values())\n",
    "    if s_x == 0:\n",
    "        return 1  # If x has no entropy, then it's perfectly predictable\n",
    "    return (s_x - s_xy) / s_x\n",
    "\n",
    "def corr_categorical(df, vars = []):\n",
    "  if vars == []:\n",
    "    vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "  dfcat = df[vars]\n",
    "\n",
    "  correlations = {}\n",
    "\n",
    "  for i in range(len(vars)):\n",
    "    for j in range(i+1, len(vars)):\n",
    "      x = dfcat[vars[i]]\n",
    "      y = dfcat[vars[j]]\n",
    "      theils_u_value = theils_u(x, y)\n",
    "      correlations.update({(x.name, y.name): theils_u_value})\n",
    "\n",
    "  # Extract unique variable names\n",
    "  variables = set()\n",
    "  for pair in correlations.keys():\n",
    "      variables.update(pair)\n",
    "  variables = sorted(variables)  # Sort for consistent ordering\n",
    "\n",
    "  # Create an empty DataFrame\n",
    "  matrix = pd.DataFrame(np.nan, index=variables, columns=variables)\n",
    "\n",
    "  # Fill the DataFrame with the correlation values\n",
    "  for (var1, var2), corr_value in correlations.items():\n",
    "    if var1!=var2:\n",
    "      matrix.loc[var1, var2] = corr_value\n",
    "      matrix.loc[var2, var1] = corr_value  # Ensure symmetry\n",
    "\n",
    "  # Fill diagonal with 1.0 (if not already provided)\n",
    "  np.fill_diagonal(matrix.values, 1.0)\n",
    "\n",
    "  display(matrix)\n",
    "  return matrix\n",
    "\n",
    "\n",
    "def stacked_barplot(predictor_data, target_data, to_normalize=True,\n",
    "                    alpha = 0.5,\n",
    "                    rotation_x_tick = 0,\n",
    "                    height= 3):\n",
    "    \"\"\"\n",
    "    Print the category counts and plot a stacked bar chart\n",
    "\n",
    "    predictor: independent variable\n",
    "    target: target variable\n",
    "    \"\"\"\n",
    "    count = predictor_data.nunique()\n",
    "    sorter = target_data.value_counts().index[-1]\n",
    "    tab1 = pd.crosstab(predictor_data, target_data, margins=True).sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    # display(tab1)\n",
    "\n",
    "    if to_normalize:\n",
    "      tab = pd.crosstab(predictor_data, target_data, normalize=\"index\").sort_values(\n",
    "          by=sorter, ascending=False\n",
    "      )\n",
    "    else:\n",
    "      tab = pd.crosstab(predictor_data, target_data).sort_values(\n",
    "          by=sorter, ascending=False\n",
    "      )\n",
    "\n",
    "    tab.plot(kind=\"bar\", stacked=True, alpha=alpha, edgecolor='gray', figsize=(count + height, height))\n",
    "    # plt.xlabel(predictor_data.name.replace('_', '\\n'))\n",
    "    plt.xlabel(modify_varname(predictor_data.name))\n",
    "    plt.ylabel('Normalized\\nFrequency')\n",
    "    # plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False, title=target_data.name.replace('_', '\\n'))\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False, title=modify_varname(target_data.name))\n",
    "    plt.xticks(rotation=rotation_x_tick)\n",
    "    sns.despine(top = True, right = True)\n",
    "    extra = '_normalized' if to_normalize else ''\n",
    "    plt.savefig(f\"{image_dir}/stacked_barplot_{target_data.name}_vs_{predictor_data.name}{extra}.png\", bbox_inches='tight');\n",
    "\n",
    "def bivariate_categorical_plots(data, vars = []):\n",
    "  if vars == []:\n",
    "    vars = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "  vars_limited = [c for c in vars if data[c].nunique()< 10]\n",
    "\n",
    "  for i in range(len(vars_limited)):\n",
    "    for j in range(i, len(vars_limited)):\n",
    "      var1 = vars_limited[i]\n",
    "      var2 = vars_limited[j]\n",
    "      if var1!=var2:\n",
    "        stacked_barplot(data[var1], data[var2])\n",
    "\n",
    "\n",
    "def pairplot_categorical(data, vars = [], hue = None, marker = 'o', alpha = 0.5,\n",
    "                         to_normalize=True,\n",
    "                         figsize=(10, 10),\n",
    "                         rotation_x = 30,\n",
    "                         rotation_y = 90,\n",
    "                         rotation_x_tick = 0,\n",
    "                         rotation_y_tick = 0,\n",
    "                         ):\n",
    "\n",
    "  if vars == []:\n",
    "    vars = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "  vars_limited = [c for c in vars if data[c].nunique()<5]\n",
    "\n",
    "\n",
    "  modify_name = lambda x: x.replace('_', '\\n')\n",
    "\n",
    "  n_cat = len(vars_limited)\n",
    "  fig, axs = plt.subplots(n_cat, n_cat, layout=\"constrained\",\n",
    "                          figsize=figsize, sharex='col', sharey='row')\n",
    "\n",
    "  for i in range(n_cat):\n",
    "    for j in range(n_cat):\n",
    "      x = data[vars_limited[i]]\n",
    "      y = data[vars_limited[j]]\n",
    "      if j >= i:\n",
    "        if i == j:\n",
    "          tab = x.value_counts(normalize=to_normalize)\n",
    "          stacked = False\n",
    "          tab.plot(kind=\"bar\", stacked=stacked, ax = axs[j, i], facecolor='none', edgecolor='gray')\n",
    "        else:\n",
    "          stacked = True\n",
    "          if to_normalize:\n",
    "            tab = pd.crosstab(x,y, normalize=\"index\")\n",
    "          else:\n",
    "            tab = pd.crosstab(x,y)\n",
    "          tab.plot(kind=\"bar\", stacked=stacked, ax = axs[j, i], alpha=0.5, edgecolor='gray')\n",
    "\n",
    "        if j!=i:\n",
    "          if i == 0:\n",
    "            axs[j, i].legend(frameon = False, bbox_to_anchor=(-1.4, .5), loc='center')\n",
    "          else:\n",
    "            axs[j, i].get_legend().remove()\n",
    "        axs[j, i].set_xlabel(modify_varname(x.name), rotation=rotation_x)\n",
    "        axs[j, i].set_ylabel(modify_varname(y.name), rotation=rotation_y)\n",
    "        axs[j, i].tick_params(axis='x', rotation=rotation_x_tick)\n",
    "        axs[j, i].tick_params(axis='y', rotation=rotation_y_tick)\n",
    "        if i!=0:\n",
    "          axs[j, i].set_ylabel('')\n",
    "        if j != n_cat-1:\n",
    "          axs[j, i].set_xlabel('')\n",
    "        sns.despine(ax=axs[j, i], top=True, right=True)\n",
    "      else:\n",
    "        sns.despine(ax=axs[j, i], top=True, right=True, left=True, bottom=True)\n",
    "        axs[j, i].axis('off')\n",
    "\n",
    "  plt.savefig(f\"{image_dir}/pairplot_categorical.png\", bbox_inches='tight');\n",
    "\n",
    "\n",
    "def bivariate_category_effect(cat1, cat2, alpha = 0.5):\n",
    "  crosstab = pd.crosstab(cat1, cat2)\n",
    "  chi, p_value, dof, expected =  stats.chi2_contingency(crosstab)\n",
    "  Ho = f\"{cat1.name} has NO effect on {cat2.name}\"\n",
    "  Ha = f\"{cat1.name} HAS AN EFFECT on {cat2.name}\"\n",
    "\n",
    "  has_effect = False\n",
    "\n",
    "  if p_value < alpha:  # Setting our significance level at 5%\n",
    "      t = f'{Ha} as the p_value {p_value:.1e} < {alpha}.'\n",
    "      has_effect = True\n",
    "  else:\n",
    "      t = f'{Ho} as the p_value {p_value:.1e} > {alpha}.'\n",
    "\n",
    "  return has_effect, p_value, t\n",
    "\n",
    "def get_chi_contingency(df, categorical_cols, alpha = 0.05):\n",
    "  chi_dict = {}\n",
    "  for i in range(len(categorical_cols)):\n",
    "    for j in range(i+1, len(categorical_cols)):\n",
    "      cat1 = df[categorical_cols[i]]\n",
    "      cat2 = df[categorical_cols[j]]\n",
    "      has_effect, p_value, t = bivariate_category_effect(cat1, cat2, alpha=alpha)\n",
    "      chi_dict.update({(cat1.name, cat2.name): {'p-value': p_value, 'Explanation': t}})\n",
    "\n",
    "  chi_df = (\n",
    "                              pd.DataFrame(chi_dict)\n",
    "                              .T.reset_index()\n",
    "                              .sort_values(by='p-value', ascending=True)\n",
    "                              .reset_index(drop=True)\n",
    "                              )\n",
    "  chi_df.columns = ['Category 1', 'Category 2', 'p-value', 'Explanation']\n",
    "  chi_df = chi_df[chi_df['p-value']< alpha]\n",
    "  chi_df['p-value'] = chi_df['p-value'].apply(lambda x: f'{x:.1e}')\n",
    "  for i, row in chi_df.iterrows():\n",
    "    print(row['Explanation'])\n",
    "  display(chi_df.drop('Explanation', axis=1))\n",
    "\n",
    "\n",
    "def bivariate_categorical(df, categorical_cols, figsize=(4,4), color = 'skyblue',\n",
    "                        alpha = .3):\n",
    "  if len(categorical_cols) > 1:\n",
    "    pairplot_categorical(df, rotation_x_tick=90, rotation_x=0, figsize=(13, 13))\n",
    "    bivariate_categorical_plots(df)\n",
    "    corr = corr_categorical(df)\n",
    "    get_chi_contingency(df, categorical_cols)\n",
    "    heatmap(corr, figsize=(18, 10), corner = True, name='corr_categorical', rotation_y=0)\n",
    "\n",
    "\n",
    "def histplot_boxplot_hue(data, var,\n",
    "             figsize = (9, 5), height_ratios = [3, 2],\n",
    "             round_by=2,\n",
    "             kde = True,\n",
    "             hue = None,\n",
    "             mean_color='blue',\n",
    "             median_color='gray',\n",
    "             palette='Blues',\n",
    "             alpha_hist = .3,\n",
    "             alpha_box = .3,\n",
    "             showmeans=True):\n",
    "\n",
    "  if hue:\n",
    "    height_ratios = [3, data[hue].nunique()-1]\n",
    "\n",
    "  name = var.replace('_', '\\n')\n",
    "\n",
    "  # Create subplots with shared x-axis\n",
    "  fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True,\n",
    "                           figsize=figsize,\n",
    "                           gridspec_kw={'height_ratios': height_ratios })\n",
    "\n",
    "  # Plot the histogram\n",
    "  sns.histplot(data = data,\n",
    "               x = var,\n",
    "               kde=kde, ax=axes[0],\n",
    "               palette = palette,\n",
    "               alpha=alpha_hist,\n",
    "               hue = hue,\n",
    "               stat = 'density',\n",
    "               edgecolor='black', linewidth=.5)\n",
    "\n",
    "  title = f\"{var.replace('_', ' ')} & {hue.replace('_', ' ')}\" if hue else f\"{var.replace('_', ' ')}\"\n",
    "  # plt.suptitle(\"Histogram & Boxplot\")\n",
    "  axes[0].set_title(title)\n",
    "  axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "  # Plot the boxplot\n",
    "  sns.boxplot(data = data, x=var, ax=axes[1], width=0.5, palette=palette,\n",
    "              showmeans = showmeans,\n",
    "              hue = hue,\n",
    "              boxprops=dict(alpha=alpha_box),\n",
    "              meanprops={\"marker\": \"^\", \"markerfacecolor\": mean_color, \"markeredgecolor\": \"black\"},\n",
    "              flierprops={\"marker\": \"x\", \"markersize\": 1, \"markerfacecolor\": \"gray\"})\n",
    "  axes[1].set_xlabel(var.replace('_', '\\n'))\n",
    "\n",
    "  sns.despine(ax=axes[0])\n",
    "  sns.despine(ax=axes[1])\n",
    "\n",
    "\n",
    "\n",
    "  axes[0].get_legend().remove()\n",
    "  if hue:\n",
    "    axes[1].legend(frameon=False, loc='upper right', bbox_to_anchor=(1.5, 1),\n",
    "                   fontsize = 12,\n",
    "                   title=hue.replace('_', '\\n'))\n",
    "\n",
    "\n",
    "  # Adjust layout and show\n",
    "  plt.tight_layout()\n",
    "  extra = f'_vs_{hue}' if hue else ''\n",
    "  plt.savefig(f\"{image_dir}/histplot_boxplot_{name}{extra}.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "def bivariate_categorical_numerical_effect(num_data, cat_data, alpha = 0.05):\n",
    "  cat_values = cat_data.unique().tolist()\n",
    "\n",
    "  groups = [num_data[cat_data == value] for value in cat_values]\n",
    "\n",
    "\n",
    "  Ho = f\"{cat_data.name} has no effect on {num_data.name}\"\n",
    "  Ha = f\"{cat_data.name} HAS AN EFFECT on {num_data.name}\"\n",
    "\n",
    "  if len(groups) == 2:\n",
    "    test_stat, p_value  = stats.ttest_ind(*groups)\n",
    "    method = 'Two-Sample T-Test'\n",
    "  else:\n",
    "    test_stat, p_value = stats.f_oneway(*groups)\n",
    "    method = 'One-Way ANOVA F-test'\n",
    "\n",
    "  has_effect = False\n",
    "\n",
    "  if p_value < alpha:\n",
    "      has_effect = True\n",
    "      t = f'{Ha} as the {method} p_value {p_value:.1e} < {alpha}.'\n",
    "  else:\n",
    "      t = f'{Ho} as the {method} p_value {p_value:.1e} > {alpha}.'\n",
    "\n",
    "  return has_effect, p_value, t\n",
    "\n",
    "\n",
    "def get_pvalue_table_categorical_numerical(df, categorical_cols, numerical_cols, alpha = 0.05):\n",
    "  pdict = {}\n",
    "  for cat_col in categorical_cols:\n",
    "    for num_col in numerical_cols:\n",
    "      cat_data = df[cat_col]\n",
    "      num_data = df[num_col]\n",
    "      has_effect, p_value, t = bivariate_categorical_numerical_effect(num_data, cat_data, alpha=alpha)\n",
    "      pdict.update({(cat_col, num_col): {'p-value': p_value, 'Explanation': t}})\n",
    "\n",
    "  p_df = (\n",
    "          pd.DataFrame(pdict)\n",
    "          .T.reset_index()\n",
    "          .sort_values(by='p-value', ascending=True)\n",
    "          .reset_index(drop=True)\n",
    "  )\n",
    "  p_df.columns = ['Category', 'Numerical', 'p-value', 'Explanation']\n",
    "  p_df = p_df[p_df['p-value']< alpha]\n",
    "  p_df['p-value'] = p_df['p-value'].apply(lambda x: f'{x:.1e}')\n",
    "  for i, row in p_df.iterrows():\n",
    "    print(row['Explanation'])\n",
    "  display(p_df.drop('Explanation', axis=1))\n",
    "\n",
    "def bivariate_numerical_categorical(df, categorical_cols, numerical_cols, palette_dict_cat):\n",
    "  get_pvalue_table_categorical_numerical(df, categorical_cols, numerical_cols)\n",
    "  for cat in categorical_cols:\n",
    "    for num in numerical_cols:\n",
    "      palette = palette_dict_cat[cat]\n",
    "      histplot_boxplot_hue(df, var=num, hue=cat, palette=palette)\n",
    "\n",
    "\n",
    "def clustered_heatmap(corr):\n",
    "  from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "\n",
    "  # Compute correlation matrix\n",
    "  correlation_matrix = corr\n",
    "\n",
    "  # Use hierarchical clustering to reorder rows and columns\n",
    "  linkage_matrix = linkage(correlation_matrix, method='average')\n",
    "  ordered_indices = leaves_list(linkage_matrix)\n",
    "\n",
    "  # Reorder the correlation matrix\n",
    "  correlation_matrix_sorted = correlation_matrix.iloc[ordered_indices, ordered_indices]\n",
    "\n",
    "\n",
    "  heatmap(correlation_matrix_sorted,figsize=(18, 10), vmin = -1 , vmax = 1, corner = True,\n",
    "              rotation_x = 45, rotation_y = 0, annot=False, to_rename=False,\n",
    "              tick_fontsize = 11,\n",
    "              name='all_columns_clustered')\n",
    "\n",
    "\n",
    "# sort_columns\n",
    "def get_numerical_categorical_info(df):\n",
    "  df = df[df.columns.sort_values().tolist()]\n",
    "  categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "  numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "  palettes_cat = [\n",
    "    \"Blues\", \"Greens\", \"Reds\", \"Purples\", \"Oranges\", \"Greys\",\n",
    "    \"YlGn\", \"YlGnBu\", \"GnBu\", \"BuGn\", \"PuBu\", \"PuBuGn\", \"BuPu\",\n",
    "    \"OrRd\", \"PuRd\", \"RdPu\", \"YlOrBr\", \"YlOrRd\", 'rocket', 'mako', 'flare', 'crest'\n",
    "  ]\n",
    "  colors_num = ['blue', 'green', 'red', 'orange', 'purple', 'dodgerblue', 'gray'] + ['steelblue', 'skyblue', 'seagreen', 'limegreen', 'lightgreen', 'firebrick', 'tomato', 'indianred', 'darkorange', 'gold', 'mediumpurple',\n",
    "                                            'orchid', 'yellow', 'gold', 'khaki', 'lightgray', 'darkgray', 'silver']\n",
    "\n",
    "  if len(colors_num) < len(numerical_cols):\n",
    "    print(f'Add {len(numerical_cols) - len(colors_num)} colors.')\n",
    "\n",
    "  if len(palettes_cat) < len(categorical_cols):\n",
    "    print(f'Add {len(categorical_cols) - len(palettes_cat)} palettes.')\n",
    "\n",
    "  categorical_cols_missing = [c for c in categorical_cols if f'{c} (missing)' in categorical_cols]\n",
    "  numerical_cols_missing = [c for c in numerical_cols if f'{c} (missing)' in numerical_cols]\n",
    "\n",
    "  palette_dict_cat = {k: v for k, v in zip(categorical_cols, palettes_cat)}\n",
    "  palette_dict_cat.update({k: palette_dict_cat[k.split(' (missing)')[0]] for k in palette_dict_cat if '(missing)' in k})\n",
    "  colors_dict_num = {k: v for k, v in zip(numerical_cols, colors_num) if '> 0' not in k}\n",
    "  colors_dict_num.update({k: colors_dict_num[k.split(' > 0')[0]] for k in numerical_cols if '> 0' in k})\n",
    "  colors_dict_num.update({k: colors_dict_num[k.split(' (missing)')[0]] for k in numerical_cols if '(missing)' in k})\n",
    "  return df, categorical_cols, numerical_cols, palette_dict_cat, colors_dict_num\n",
    "\n",
    "\n",
    "def plot_all_univariate(df, categorical_cols, numerical_cols, palette_dict_cat, colors_dict_num, thresh = 10):\n",
    "  for col in categorical_cols:\n",
    "    palette = palette_dict_cat[col]\n",
    "    nmax = None\n",
    "    if df[col].nunique() > thresh:\n",
    "      nmax = 9\n",
    "    univariate_categorical(df[col], palette = palette, nmax = nmax)\n",
    "\n",
    "  for col in numerical_cols:\n",
    "    color = colors_dict_num[col]\n",
    "    univariate_numerical(df[col], color)\n",
    "\n",
    "\n",
    "def plot_all_bivariate(df, categorical_cols, numerical_cols, palette_dict_cat):\n",
    "  bivariate_numerical(df, numerical_cols)\n",
    "  bivariate_categorical(df, categorical_cols)\n",
    "  bivariate_numerical_categorical(df, categorical_cols, numerical_cols, palette_dict_cat)\n",
    "  get_pvalue_table_categorical_numerical(df, categorical_cols, numerical_cols )\n",
    "\n",
    "def create_info_conduct_eda(data, positive_cols=[], drop_cols=[], type_conv={},\n",
    "                  feature_info={}, modify_dict={}, ddict_str=\"\", cat_orders={}, to_plot = False):\n",
    "  data = create_info(data, positive_cols = positive_cols,\n",
    "                        type_conv = type_conv,\n",
    "                        feature_info = feature_info,\n",
    "                  modify_dict = modify_dict,\n",
    "                            drop_cols = drop_cols, ddict_str = ddict_str, cat_orders=cat_orders)\n",
    "\n",
    "  data, categorical_cols, numerical_cols, palette_dict_cat, colors_dict_num = get_numerical_categorical_info(data)\n",
    "\n",
    "  if to_plot:\n",
    "    missing_data_cols = [c.split(\" (missing)\")[0] for c in data.columns if '(missing)' in c]\n",
    "    for c in missing_data_cols:\n",
    "      data[f'{c} (missing)'] = data[f'{c} (missing)'].fillna('Unknown')\n",
    "      data[f'{c} (missing)'] = pd.Categorical(data[f'{c} (missing)'], categories=['Unknown']+ cat_orders[c], ordered=True)\n",
    "    plot_all_univariate(data, categorical_cols, numerical_cols, palette_dict_cat, colors_dict_num)\n",
    "    categorical_cols = [c for c in categorical_cols if c not in missing_data_cols]\n",
    "    numerical_cols = [c for c in numerical_cols if c not in missing_data_cols]\n",
    "    data_drop_extra = data.drop(missing_data_cols, axis = 1)\n",
    "    plot_all_bivariate(data_drop_extra, categorical_cols, numerical_cols, palette_dict_cat)\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0c94b-8503-4877-805c-19e7aea1006e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
